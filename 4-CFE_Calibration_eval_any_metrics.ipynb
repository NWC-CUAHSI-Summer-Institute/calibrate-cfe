{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate desired evaluation metrics from best run timeseries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import hydroeval as he\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import hydroeval as he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nse(modeled_data, observed_data):\n",
    "    mean_observed = np.mean(observed_data)\n",
    "    numerator = np.sum(np.power(observed_data - modeled_data, 2))\n",
    "    denominator = np.sum(np.power(observed_data - mean_observed, 2))\n",
    "    return 1 - np.divide(numerator, denominator)\n",
    "\n",
    "def load_basin_list(basin_filename):\n",
    "    with open(basin_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Remove leading/trailing whitespaces and newline characters\n",
    "        lines = [line.strip() for line in lines]\n",
    "    basin_list_str = lines\n",
    "    return basin_list_str\n",
    "\n",
    "def load_time_index(time_split_file):\n",
    "    with open(time_split_file) as f:\n",
    "        time_split = json.load(f)\n",
    "        print(time_split)\n",
    "        start_time = time_split['calibration']['start_datetime']\n",
    "        end_time = time_split['calibration']['end_datetime']\n",
    "        hourly_index = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "    return hourly_index, start_time, end_time\n",
    "\n",
    "def load_best_run_timeseries(best_runs_dir, basin_id, hourly_index): # Get the file names in the directory\n",
    "    best_runs_files = os.listdir(best_runs_dir)\n",
    "    best_run_for_a_basin = [file_name for file_name in best_runs_files if file_name.startswith(basin_id)]\n",
    "    if len(best_run_for_a_basin)==1:\n",
    "        with open(os.path.join(best_runs_dir, best_run_for_a_basin[0])) as f:\n",
    "            best_run = json.load(f)\n",
    "    elif len(best_run_for_a_basin)==0:\n",
    "        warnings.warn(\"No calibration was done for this basin\")\n",
    "    elif len(best_run_for_a_basin)>1:\n",
    "        warnings.warn(\"Multiple calibration runs are mixed up in one folder, check\")\n",
    "    best_run_timeseries = pd.DataFrame(best_run['best simulation results'], columns=['simulated'])\n",
    "    best_run_timeseries.set_index(hourly_index, inplace=True)\n",
    "    return best_run_timeseries\n",
    "    \n",
    "def load_obs_timeseries(obs_data_dir, start_time, end_time, basin_id):\n",
    "    obs_data_files = os.listdir(obs_data_dir)\n",
    "    obs_data_file = [file_name for file_name in obs_data_files if file_name.startswith(basin_id)]\n",
    "\n",
    "    if len(obs_data_file)==1:\n",
    "        obs_data_ = pd.read_csv(os.path.join(obs_data_dir, obs_data_file[0]))\n",
    "    else:\n",
    "        warnings.warn(\"data is gone!\")\n",
    "        \n",
    "    obs_data_['date'] = pd.to_datetime(obs_data_['date'])\n",
    "    obs_data_.set_index(obs_data_['date'], inplace=True)\n",
    "    obs_data = obs_data_[start_time:end_time]\n",
    "    \n",
    "    return obs_data\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the config file\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access the config variables\n",
    "results_dir = config['io_dir']['results_dir'].replace(\"${cwd}\", \"..\")\n",
    "best_runs_dir = os.path.join(results_dir, 'best_runs')\n",
    "basin_filename = config['model_settings']['basin_file'].replace(\"${cwd}\", \"..\")\n",
    "time_split_file = config['model_settings']['time_split_file'].replace(\"${cwd}\", \"..\")\n",
    "obs_data_dir = config['io_dir']['usgs_streamflow_dir'].replace(\"${cwd}\", \"..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through basins and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spinup-for-calibration': {'start_datetime': '1998-10-01 00:00:00', 'end_datetime': '1999-09-30 23:00:00', 'note': 'Used for CFE model spin-up by 2023 team (1yr before calibration period)'}, 'calibration': {'start_datetime': '1999-10-01 00:00:00', 'end_datetime': '2008-09-30 23:00:00', 'note': 'Used for calibrating CFE model & training LSTM by 2023 team. Based on previous studies (Frame et al., 2022; Kratzert et al., 2019; Hoedt et al., 2021; Kratzert et al., 2021)'}, 'spinup-for-testing': {'start_datetime': '2008-10-01 00:00:00', 'end_datetime': '2009-09-30 23:00:00', 'note': 'Used for CFE model spin-up by 2023 team (1yr before testing period)'}, 'testing': {'start_datetime': '2009-10-01 00:00:00', 'end_datetime': '2010-09-30 23:00:00', 'note': \"Used for checking CFE & LSTM performance after calibration by 2023 team. No validation (hyper-parameter tuning) performed for LSTM. 1990 doesn't have much data\"}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected 52608 rows, received array of length 78912",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mR:\\Temp\\ipykernel_10316\\659604362.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Load time split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mhourly_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_time_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_split_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Load and combine obs & simulated data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mbest_run_timeseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_best_run_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_runs_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_runs_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasin_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbasin_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhourly_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhourly_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mobs_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_obs_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_data_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobs_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasin_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbasin_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdf_eval_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run_timeseries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Temp\\ipykernel_10316\\2139577366.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(best_runs_dir, basin_id, hourly_index)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No calibration was done for this basin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run_for_a_basin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Multiple calibration runs are mixed up in one folder, check\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mbest_run_timeseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'best simulation results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'simulated'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mbest_run_timeseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhourly_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_run_timeseries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flipl\\miniconda3\\envs\\CFE-torch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   5906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5908\u001b[0m                 \u001b[1;31m# check newest element against length of calling frame, since\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5909\u001b[0m                 \u001b[1;31m# ensure_index_from_sequences would not raise for append=False.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5910\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   5911\u001b[0m                     \u001b[1;34mf\"Length mismatch: Expected {len(self)} rows, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5912\u001b[0m                     \u001b[1;34mf\"received array of length {len(arrays[-1])}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5913\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected 52608 rows, received array of length 78912"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "basin_list_str = load_basin_list(basin_filename)\n",
    "df_results = pd.DataFrame(columns=['BasinID', 'NSE', 'NNSE', 'logNSE', 'KGE', 'logKGE'])\n",
    "plot_results = False\n",
    "\n",
    "# # To test one basin\n",
    "# i=0\n",
    "# basin_id = basin_list_str[i]\n",
    "# for basin_id in ([basin_list_str[i]]):\n",
    "\n",
    "# Loop through all the basins\n",
    "for basin_id in tqdm(basin_list_str):\n",
    "\n",
    "    # Load time split\n",
    "    hourly_index, start_time, end_time = load_time_index(time_split_file)\n",
    "    \n",
    "    # Load and combine obs & simulated data \n",
    "    best_run_timeseries = load_best_run_timeseries(best_runs_dir=best_runs_dir, basin_id=basin_id, hourly_index=hourly_index)\n",
    "    obs_data = load_obs_timeseries(obs_data_dir=obs_data_dir, start_time=start_time, end_time=end_time, basin_id=basin_id)\n",
    "    df_eval_sim = obs_data.join(best_run_timeseries)\n",
    "    \n",
    "    # If you want to plot results\n",
    "    if plot_results: \n",
    "        print(df_eval_sim.head())\n",
    "        df_eval_sim[['QObs(mm/h)', 'simulated']].plot()\n",
    "        \n",
    "    # Get desired columns\n",
    "    modeled_data_ = df_eval_sim['simulated'].values\n",
    "    observed_data_ = df_eval_sim['QObs(mm/h)'].values\n",
    "    \n",
    "    # Skip nan values in the observed data\n",
    "    modeled_data = modeled_data_[~np.isnan(observed_data_)]\n",
    "    observed_data = observed_data_[~np.isnan(observed_data_)]\n",
    "    \n",
    "    # Evalute (you can add as many eval metrics here)\n",
    "    nse = calculate_nse(modeled_data=modeled_data, observed_data=observed_data)\n",
    "    nnse = 1/(2-nse)\n",
    "    lognse = calculate_nse(modeled_data=np.log(modeled_data), observed_data=np.log(observed_data))\n",
    "    kge = he.evaluator(he.kge, simulations=modeled_data, evaluations=observed_data)\n",
    "    logkge = he.evaluator(he.kge, simulations=np.log(modeled_data), evaluations=np.log(observed_data))\n",
    "    \n",
    "    df_results = pd.concat([df_results, pd.DataFrame({\n",
    "        'BasinID': [basin_id], \n",
    "        'NSE': [nse], \n",
    "        'NNSE':[nnse], \n",
    "        'logNSE':[lognse],\n",
    "        'KGE':[kge],\n",
    "        'logKGE': [logkge]\n",
    "        })])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "df_results = df_results.reset_index(drop=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(df_results)\n",
    "df_results.to_csv(os.path.join(best_runs_dir, 'best_runs_eval.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFE-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
